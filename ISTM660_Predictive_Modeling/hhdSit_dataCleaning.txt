# -*- coding: utf-8 -*-
"""“3B_Sample Project Data Processing and Matching” - Data Cleaning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HJDWaYT4qvKk29gDup6plba2NlW7Y-i9

**❗ PLEASE SAVE A COPY IN YOUR OWN DRIVE AND START THERE ❗**

#🥅 Objective: State-Level Analysis of Household Self-Employment Rates

## ⚙️ Environment Setup
"""

#Install BigQuery Library
!pip install pandas-gbq

#Import Libraries
import pandas as pd
from pandas.io import gbq
import io
import google.cloud.bigquery as bq
import numpy as np

"""## 🔒 Verify Your Google Credential"""

# Get project ID
project_id = 'pepperdine-istm660'

#Verify your Google credential
#Make sure you're using your school account: ***@pepperdine.edu

from google.colab import auth
auth.authenticate_user()

# Set client
client = bq.Client(project=project_id)

"""## Access BQ Tables Data Files"""

variables = 'hhid','hhid2','lineno','age','married','marstat','ownchild','ch02','ch05','ch35','ch613','ch1417','ownchild','female','hhnum','hoh79','lfstat','nilf','selfemp','selfinc','empl' #@param {type:"raw"}

#Check if variables list is empty or starts with an empty string, and fix it
if not variables or variables[0] == '':
    #If variables is empty, you need to define the columns you want to select
    variables = ['hhid','hhid2','lineno','age','married','marstat','ownchild','ch02','ch05','ch35','ch613','ch1417','ownchild','female',
                 'hhnum','hoh79','lfstat','nilf','selfemp','selfinc','empl'] # Replace with your desired columns
    # or if it starts with an empty string:
    # variables = variables[1:] #This removes the first element if it's empty

"""#2018"""

# #Load data
# dataset_id = 'CPS' #@param {type:"string"}
# table_id = 'cepr_org_2018' #@param {type:"string"}

# # Replace 'emp1' with 'empl' or the correct column name in the 'variables' list
# variables = [var.replace('emp1', 'empl') for var in variables]

# # After the correction, regenerate the query string
# variables_str = ', '.join([f"`{var}`" for var in variables])
# query = f"""
#     SELECT {variables_str}
#     FROM `{project_id}.{dataset_id}.{table_id}`
# """

# #Store data as dataframe
# df1 = client.query(query).to_dataframe()

# df1

"""#2019"""

# #Load data
# dataset_id = 'CPS' #@param {type:"string"}
# table_id = 'cepr_org_2019' #@param {type:"string"}

# variables_str = ', '.join([f"`{var}`" for var in variables])
# query = f"""
#     SELECT {variables_str}
#     FROM `{project_id}.{dataset_id}.{table_id}`
# """

# df2 = client.query(query).to_dataframe()

# df2

"""#Alternative and more efficient"""

dataset_id = 'CPS' #@param {type:"string"}

# List of table names for each year (replace with your actual table names)
table_names = [
    "cepr_org_2009",
    "cepr_org_2010",
    "cepr_org_2011",
    "cepr_org_2012",
    "cepr_org_2013",
    "cepr_org_2014",
    "cepr_org_2015",
    "cepr_org_2016",
    "cepr_org_2017",
    "cepr_org_2018",
    "cepr_org_2019"
]

# List of variables
variables = [
    "hhid", "hhid2", "lineno", "age", "married", "marstat","ownchild",
    "ch02", "ch05", "ch35", "ch613", "ch1417",
    "female", "hhnum", "hoh79", "lfstat", "nilf",
    "selfemp", "selfinc", "empl", "unem"
]

# Dynamically construct the variable string with backticks
variables_str = ', '.join([f"`{var}`" for var in variables])

# Construct the query dynamically for all tables with stacked results
query = " UNION ALL ".join([
    f"SELECT {variables_str}, '{table_name[-4:]}' AS year FROM `pepperdine-istm660.CPS.{table_name}`"
    for table_name in table_names
])

# Run the query and pull the result into a DataFrame
query_job = client.query(query)
stacked_df = query_job.to_dataframe()

# Concatenate to create a primary key
stacked_df['hhhid_full'] = stacked_df['hhid'].astype(str) + '-' + stacked_df['hhid2'].astype(str)

# Display the resulting stacked DataFrame
print(stacked_df)

"""## Data Processing: Merging and Filtering"""

# df1.info()

# df2.info()

#Add full household id
stacked_df['hhhid_full'] = stacked_df['hhid'].astype(str) + '-' + stacked_df['hhid2'].astype(str)

# List of DataFrames to stack
dataframes = stacked_df[stacked_df['year'] == '2009'], stacked_df[stacked_df['year'] == '2010'], stacked_df[stacked_df['year'] == '2011'], stacked_df[stacked_df['year'] == '2012'], stacked_df[stacked_df['year'] == '2013'], stacked_df[stacked_df['year'] == '2014'], stacked_df[stacked_df['year'] == '2015'], stacked_df[stacked_df['year'] == '2016'], stacked_df[stacked_df['year'] == '2017'], stacked_df[stacked_df['year'] == '2018'], stacked_df[stacked_df['year'] == '2019']

# Stack all DataFrames vertically
stacked_df = pd.concat(dataframes, ignore_index=True)

# Display the result
print(stacked_df)

"""#Important! Convert the character to numnerics"""

stacked_df.info()

import pandas as pd

# Ensure columns to convert are selected properly
cols_to_convert = stacked_df.columns.difference(['hhid', 'hhid2'])

# Apply numeric conversion explicitly
stacked_df[cols_to_convert] = stacked_df[cols_to_convert].apply(lambda x: pd.to_numeric(x, errors='coerce'))

# Verify the data types
print(stacked_df.dtypes)

#Copied from https://drive.google.com/drive/folders/1v1njYnuowPMRaECCiI5c9a0fjtWTX9jM

# states = {11: "Maine",
#           12: "New Hampshire",
#           13: "Vermont",
#           14: "Massachusetts",
#           15: "Rhode Island",
#           16: "Connecticut",
#           21: "New York",
#           22: "New Jersey",
#           23: "Pennsylvania",
#           31: "Ohio",
#           32: "Indiana",
#           33: "Illinois",
#           34: "Michigan",
#           35: "Wisconsin",
#           41: "Minnesota",
#           42: "Iowa",
#           43: "Missouri",
#           44: "North Dakota",
#           45: "South Dakota",
#           46: "Nebraska",
#           47: "Kansas",
#           51: "Delaware",
#           52: "Maryland",
#           53: "District of Columbia",
#           54: "Virginia",
#           55: "West Virginia",
#           56: "North Carolina",
#           57: "South Carolina",
#           58: "Georgia",
#           59: "Florida",
#           61: "Kentucky",
#           62: "Tennessee",
#           63: "Alabama",
#           64: "Mississippi",
#           71: "Arkansas",
#           72: "Louisiana",
#           73: "Oklahoma",
#           74: "Texas",
#           81: "Montana",
#           82: "Idaho",
#           83: "Wyoming",
#           84: "Colorado",
#           85: "New Mexico",
#           86: "Arizona",
#           87: "Utah",
#           88: "Nevada",
#           91: "Washington",
#           92: "Oregon",
#           93: "California",
#           94: "Alaska",
#           95: "Hawaii"}

# Add full household id
stacked_df['hhhid_full'] = stacked_df['hhid'].astype(str) + '-' + stacked_df['hhid2'].astype(str)

stacked_df

# # Map the 'state_code' to state names using the dictionary
# stacked_df['state_descr'] = stacked_df['state'].map(states)

#drop variables
stacked_df = stacked_df.drop(['hhid','hhid2'], axis=1)

# Move column 'B' to the front
col_to_move = 'hhhid_full'
new_order = [col_to_move] + [col for col in stacked_df.columns if col != col_to_move]
stacked_df = stacked_df.reindex(columns=new_order)

stacked_df

stacked_df.info()

# Drop rows with NA values in 'selfemp' and 'selfinc' columns
stacked_df = stacked_df.dropna(subset=['selfemp', 'selfinc'])

# Convert 'self_employment', 'selfemp', and 'selfinc' to integers
stacked_df['selfemp'] = pd.to_numeric(stacked_df['selfemp'], errors='coerce').astype('Int64')
stacked_df['selfinc'] = pd.to_numeric(stacked_df['selfinc'], errors='coerce').astype('Int64')

# Combine 'selfemp' and 'selfinc' into a new column 'self_employment'
stacked_df['self_employment'] = (stacked_df['selfemp'] | stacked_df['selfinc']).astype(int)

# Drop the original columns
stacked_df = stacked_df.drop(['selfemp', 'selfinc'], axis=1)

# Move 'self_employment' to the 4th column
cols = list(stacked_df.columns)
cols.insert(3, cols.pop(cols.index('self_employment')))
stacked_df = stacked_df.loc[:, cols]

stacked_df

"""#Merge based on criteria"""

# # Separate the data for marstat==1 and marstat==3
# ms_1 = stacked_df[stacked_df['marstat'] == 1].add_suffix('_ms1')  # Add suffix to marstat 1 column
# ms_3 = stacked_df[stacked_df['marstat'] == 3].add_suffix('_ms3')

# ms_1

# ms_3

# # Remove the 'ms1' and 'ms3' suffix from the shared columns (HHID, HHID2, line_number, and age)
# ms_1 = ms_1.rename(columns={
#     'hhhid_full_ms1': 'hhhid_full',
#     'lineno_ms1': 'lineno',
#     'age_ms1': 'age'
# })

# ms_3 = ms_3.rename(columns={
#     'hhhid_full_ms3': 'hhhid_full',
#     'lineno_ms3': 'lineno',
#     'age_ms3': 'age'
# })

# ms_1

# ms_3

# # Perform a cross join to match all possible rows
# merged_df = pd.merge(ms_1,ms_3, on=['hhhid_full', 'lineno'], how='outer')

# merged_df

# merged_df.info()

# # Merge 'age_ms1' and 'age_ms3' into a single 'age' column.
# # Prioritize 'age_ms1' if both are present, otherwise use 'age_ms3'.
# merged_df['age'] = merged_df['age_x'].combine_first(merged_df['age_y'])

# # Drop the original 'age' columns
# merged_df = merged_df.drop(['age_x', 'age_y'], axis=1)

# merged_df.info()

# # Move column 'age' to index 2
# cols = list(merged_df.columns)
# cols.insert(2, cols.pop(cols.index('age')))
# merged_df = merged_df.loc[:, cols]
# merged_df

# Since ch05 can reflect the status of ch02 and ch35
# Drop the 'ch02' column
stacked_df = stacked_df.drop('ch02', axis=1)
stacked_df = stacked_df.drop('ch35', axis=1)

stacked_df = stacked_df[stacked_df['age'] >= 18]

# filtered_df.info()

# filtered_df

# reset the index of stacked_df
stacked_df = stacked_df.reset_index(drop=True)
stacked_df

# Define the columns to replace with mode
columns_to_replace = ['ch05','ch613','ch1417','ownchild']

# Iterate through the columns and replace missing values with the mode
for column in columns_to_replace:
    if column in stacked_df.columns:  # Check if the column exists in the DataFrame
        mode_value = stacked_df[column].mode()[0]
        stacked_df[column] = stacked_df[column].fillna(mode_value)

stacked_df.isnull().sum()

"""#Upload your team data onto bigquery"""

dataset_id = "CPS"  # Replace with your dataset name - always CPS
table_id = "HouseholdSituation"  # Replace with your table name - of our choice

# Full table path
table_path = f"{'pepperdine-istm660'}.{'CPS'}.{'HouseholdSituation'}"

stacked_df.to_gbq(destination_table=table_path,
          project_id=project_id,
          if_exists="replace")  # "replace", "append", or "fail"

"""#Verify it was uploaded correctly"""

query = f"SELECT * FROM `{table_path}` LIMIT 10"
df_check = client.query(query).to_dataframe()
print(df_check)

"""**Project: Analyze Project Data and develop Project Plan (Team)**

- This assignment is intended to combine your findings and analysis with your team members while continuing to make progress on the project.
- You include a summary of the available data including unique record count.
- You should define the business problem/questions based on the team interpretation.
- Additionally, you should identify key data sources and variables that would allow you to address the business problem/questions.
- You should develop testable hypotheses and discuss how you plan to test them.
- This should include what additional processing of the data needs to occur (if you started processing the data you should include what assumptions, filters, etc. you applied to settle on the final population for each data source).
- You should include what software tools you plan to use
- Finally, you should include next steps and which team members are going to be responsible for completion.
- This should be a 10-minute presentation to be done in class (no slide minimum or maximum).

# **Hypotheses Testing**
"""

# count the null values of each column in stacked_df

null_counts = stacked_df.isnull().sum()
null_counts

# Define the columns to replace with mode
columns_to_replace = ['ch05','ch613','ch1417','ownchild']

# Iterate through the columns and replace missing values with the mode
for column in columns_to_replace:
    if column in stacked_df.columns:  # Check if the column exists in the DataFrame
        mode_value = stacked_df[column].mode()[0]
        stacked_df[column] = stacked_df[column].fillna(mode_value)

stacked_df.isnull().sum()

import statsmodels.api as sm

# Define the independent variables (features)
independent_variables = ['age', 'ch05', 'ch613', 'ch1417', 'hoh79', 'ownchild', 'hhnum', 'married']

# Define the dependent variable (target)
dependent_variable = 'self_employment'

# Prepare the data
X = stacked_df[independent_variables]
y = stacked_df[dependent_variable]

# Add a constant to the independent variables (intercept term)
X = sm.add_constant(X)

# Create and fit the logistic regression model
model = sm.Logit(y, X).fit()

# Print the model summary
print(model.summary())

# # Generate a  correlation matrix
# import pandas as pd
# import numpy as np

# # Calculate the correlation matrix
# correlation_matrix = stacked_df.corr()

# # Display the correlation matrix
# correlation_matrix

# Access the model's accuracy (pseudo-R-squared)
pseudo_r_squared = model.prsquared

print(f"Pseudo R-squared (McFadden's R-squared): {pseudo_r_squared}")

# Get predicted probabilities
predicted_probabilities = model.predict(X)

# Convert probabilities to binary predictions (e.g., using a threshold of 0.5)
predicted_classes = (predicted_probabilities > 0.5).astype(int)

# Compare predicted classes with actual classes
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
accuracy = accuracy_score(y, predicted_classes)
print(f"Accuracy: {accuracy}")

print("Classification Report:")
print(classification_report(y, predicted_classes))

print("Confusion Matrix:")
print(confusion_matrix(y, predicted_classes))

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
import shap

# Calculate ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y, predicted_probabilities)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()


# SHAP values
explainer = shap.Explainer(model.predict, X) # Use the model's predict method
shap_values = explainer(X)

# Summary plot
shap.summary_plot(shap_values, X)

# Individual force plot (example for the first observation)
shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])

# import pandas as pd
# import statsmodels.api as sm
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score, roc_auc_score

# # List of the independent variables you want to include in the model
# independent_vars = ['ch05', 'ch613', 'ch1417', 'hoh79', 'ownchild', 'hhnum', 'married']

# # Create dummy variables for categorical columns
# filtered_df_dummies = pd.get_dummies(filtered_df[independent_vars], drop_first=True)

# # Concatenate the dummy variables back to the original DataFrame
# filtered_df = pd.concat([filtered_df, filtered_df_dummies], axis=1)

# # Drop the original categorical columns
# filtered_df.drop(columns=independent_vars, inplace=True)

# # Now we need to define the dependent and independent variables
# # Let's assume that 'self_employment' is your dependent variable
# Xd = filtered_df.drop(columns=['self_employment_ms3'])  # Independent variables (changed from X to Xd)
# y = filtered_df['self_employment_ms3']  # Dependent variable

# # Add a constant to the independent variables (for intercept in logistic regression)
# Xd = sm.add_constant(Xd)

# # Split the data into training and test sets
# Xd_train, Xd_test, y_train, y_test = train_test_split(Xd, y, test_size=0.2, random_state=42)

# # Fit the logistic regression model
# model = sm.Logit(y_train, Xd_train).fit()

# # Print the model summary
# print(model.summary())

# # You can now make predictions on the test set
# y_pred = model.predict(Xd_test)

# # You can also calculate the accuracy or any other metric, like ROC AUC
# # Convert predictions to binary outcomes
# y_pred_binary = [1 if pred >= 0.5 else 0 for pred in y_pred]

# # Accuracy
# accuracy = accuracy_score(y_test, y_pred_binary)
# print(f"Accuracy: {accuracy:.4f}")

# # ROC AUC
# roc_auc = roc_auc_score(y_test, y_pred)
# print(f"ROC AUC: {roc_auc:.4f}")